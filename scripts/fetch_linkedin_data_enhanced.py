import json
import os
from datetime import datetime
import requests
from bs4 import BeautifulSoup


FALLBACK_DATA = {
    "profile": {
        "name": "Ot√°vio Ribeiro",
        "headline": "Liderando a transforma√ß√£o da jornada de Dados & AI na Educbank",
        "location": "Curitiba, Paran√°, Brasil",
        "about": """Com mais de 10 anos de experi√™ncia em Engenharia de Dados, atuo na constru√ß√£o de arquiteturas modernas e escal√°veis em nuvem, com foco em Data Lakes, Data Warehouses e plataformas de Machine Learning. Minha especializa√ß√£o envolve Databricks e seu ecossistema (Delta Lake, Unity Catalog, Delta Live Tables, Feature Store e MLflow), aplicando boas pr√°ticas de governan√ßa, automa√ß√£o e qualidade de dados em ambientes de grande escala.

Tenho forte atua√ß√£o na lideran√ßa de equipes multidisciplinares e na implementa√ß√£o de estrat√©gias data-driven, impulsionando a tomada de decis√£o baseada em dados.

Minha experi√™ncia inclui:
‚úî Modelagem de Data Warehouses (Kimball, Star Schema e Snowflake Schema) para efici√™ncia anal√≠tica.
‚úî Orquestra√ß√£o e automa√ß√£o de pipelines com Databricks Workflows, Airflow e dbt, garantindo desempenho e escalabilidade.
‚úî Governan√ßa e qualidade de dados com Unity Catalog, Delta Expectations e valida√ß√µes automatizadas.
‚úî Experi√™ncia em Google Cloud Platform (GCP) e Azure, integrando servi√ßos nativos de Big Data e AI.
‚úî Lideran√ßa t√©cnica e gest√£o de times, promovendo cultura data-driven e DevOps, al√©m de capacita√ß√£o em engenharia de dados avan√ßada.

Estou sempre em busca de desafios onde possa aplicar minha experi√™ncia em Big Data, governan√ßa e plataformas em nuvem, desenvolvendo solu√ß√µes escal√°veis, perform√°ticas e sustent√°veis.

üì© Interessado em trocar ideias sobre inova√ß√£o em engenharia de dados e Databricks? Vamos conversar!""",
        "updated_at": datetime.utcnow().isoformat() + "Z",
    },
    "experience": [
        {
            "role": "Gerente de Engenharia de Dados",
            "company": "Educbank",
            "period": "Jan 2025 - Presente",
            "location": "S√£o Paulo, Brasil (Remoto)",
            "highlights": [
                "Lideran√ßa de time de engenharia respons√°vel pela transforma√ß√£o da jornada de dados",
                "Migra√ß√£o de Data Warehouse tradicional para arquitetura Lakehouse no Databricks",
                "Implementa√ß√£o de Unity Catalog, Delta Live Tables e Spark Structured Streaming",
                "Governan√ßa de dados com foco em LGPD e boas pr√°ticas cloud",
                "Engenharia de ML: valida√ß√£o, operacionaliza√ß√£o e deploy de modelos em produ√ß√£o"
            ]
        },
        {
            "role": "Especialista em Dados",
            "company": "Oto CRM",
            "period": "Jan 2024 - Jan 2025",
            "location": "Porto Alegre, Brasil (Remoto)",
            "highlights": [
                "Otimiza√ß√£o de performance em Clickhouse, MySQL e PostgreSQL",
                "Lideran√ßa de time de integra√ß√£o de dados",
                "Desenvolvimento de engines Python para ingest√£o em Data Lakes",
                "Coleta din√¢mica de dados de m√∫ltiplas fontes (bancos, APIs, plataformas)"
            ]
        },
        {
            "role": "L√≠der T√©cnico",
            "company": "DEEP ESG",
            "period": "Ago 2023 - Out 2024",
            "location": "S√£o Jos√© dos Campos, Brasil (Remoto)",
            "highlights": [
                "Lideran√ßa de equipe multidisciplinar (engenheiros de dados + backend)",
                "Modelagem de Data Warehouse com Star Schema (Kimball)",
                "Arquitetura de calculadoras de emiss√£o de CO‚ÇÇ e KPIs ESG",
                "Stack: Airflow, DataProc, Cloud Functions (GCP), PostgreSQL, BigQuery"
            ]
        },
        {
            "role": "L√≠der T√©cnico",
            "company": "HeroSpark",
            "period": "Dez 2021 - Jul 2023",
            "location": "Curitiba, Brasil",
            "highlights": [
                "Cria√ß√£o e lideran√ßa de equipe de dados do zero",
                "Implementa√ß√£o de Data Lake e Data Warehouse",
                "Pipeline de dados com valida√ß√£o e controle de qualidade",
                "Promo√ß√£o de cultura data-driven e capacita√ß√£o de analistas",
                "Stack: AWS, Metabase, PostgreSQL, Airflow"
            ]
        },
        {
            "role": "Engenheiro de Dados",
            "company": "Grupo Voalle",
            "period": "Jan 2020 - Out 2020",
            "location": "Santa Maria, Brasil",
            "highlights": [
                "Solu√ß√µes de manuten√ß√£o em massa e migra√ß√£o de dados",
                "Migra√ß√£o MariaDB ‚Üí PostgreSQL",
                "Automa√ß√£o com Shell Script e Python",
                "An√°lise de processos empresariais para aplica√ß√£o no ERP"
            ]
        },
        {
            "role": "Founder",
            "company": "AdvanceWEB",
            "period": "Jun 2016 - Jan 2020",
            "location": "√Åguas de Chapec√≥, Brasil",
            "highlights": [
                "Funda√ß√£o e gest√£o de startup de software",
                "Desenvolvimento de ImovPedidos (CRM para gest√£o de pedidos)",
                "Desenvolvimento de QTMov - Eventos (gest√£o de eventos internos)",
                "Administra√ß√£o, planejamento estrat√©gico e comercial"
            ]
        }
    ],
    "education": [
        {
            "degree": "Curso Superior em Gest√£o de Projetos de TI",
            "field": "Gest√£o de Projetos de Tecnologia da Informa√ß√£o",
            "institution": "Est√°cio",
            "period": "2024 - 2027 (em andamento)"
        },
        {
            "degree": "Tecn√≥logo",
            "field": "Sistemas para Internet / An√°lise e Desenvolvimento de Sistemas",
            "institution": "Universidade Federal de Santa Maria (UFSM)",
            "period": "2013 - 2016"
        }
    ]
}


def parse_jsonld(soup):
    """Extrai dados do JSON-LD"""
    scripts = soup.find_all("script", type="application/ld+json")
    for script in scripts:
        if not script.string:
            continue
        try:
            data = json.loads(script.string)
        except json.JSONDecodeError:
            continue
        if isinstance(data, list):
            for entry in data:
                if isinstance(entry, dict) and entry.get("@type") == "Person":
                    return entry
        if isinstance(data, dict) and data.get("@type") == "Person":
            return data
    return {}


def parse_html_fallback(soup):
    """Tenta extrair dados diretamente do HTML"""
    profile = {}
    
    # Nome
    name_selectors = [
        "h1.top-card-layout__title",
        "h1.text-heading-xlarge",
        ".pv-text-details__left-panel h1"
    ]
    for selector in name_selectors:
        element = soup.select_one(selector)
        if element:
            profile["name"] = element.get_text(strip=True)
            break
    
    # Headline
    headline_selectors = [
        ".top-card-layout__headline",
        ".text-body-medium.break-words",
        ".pv-text-details__left-panel .text-body-medium"
    ]
    for selector in headline_selectors:
        element = soup.select_one(selector)
        if element:
            profile["headline"] = element.get_text(strip=True)
            break
    
    return profile


def build_experience(person):
    """Constr√≥i lista de experi√™ncias do JSON-LD"""
    experience = []
    works_for = person.get("worksFor")
    if isinstance(works_for, dict):
        works_for = [works_for]
    if isinstance(works_for, list):
        for org in works_for:
            if not isinstance(org, dict):
                continue
            experience.append({
                "role": person.get("jobTitle") or "",
                "company": org.get("name") or "",
                "period": "",
                "location": "",
                "highlights": [],
            })
    return experience


def main():
    profile_url = os.getenv("LINKEDIN_PROFILE_URL", "https://linkedin.com/in/donotavio/")
    use_fallback = os.getenv("USE_LINKEDIN_FALLBACK", "true").lower() == "true"
    
    print(f"üì° Fetching LinkedIn profile: {profile_url}")
    
    session = requests.Session()
    session.headers.update({
        "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36",
        "Accept-Language": "pt-BR,pt;q=0.9,en-US;q=0.8,en;q=0.7",
    })

    linkedin_cookie = os.getenv("LINKEDIN_SESSION_COOKIE")
    if linkedin_cookie:
        session.cookies.set("li_at", linkedin_cookie, domain=".linkedin.com")
        print("‚úì Session cookie configured")
    else:
        print("‚ö†Ô∏è  No session cookie - using fallback data")

    try:
        response = session.get(profile_url, timeout=30)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, "html.parser")
        
        # Tentar JSON-LD
        person = parse_jsonld(soup)
        html_data = parse_html_fallback(soup)
        
        profile = {
            "name": person.get("name") or html_data.get("name") or "",
            "headline": person.get("jobTitle") or html_data.get("headline") or "",
            "location": "",
            "about": person.get("description") or "",
            "updated_at": datetime.utcnow().isoformat() + "Z",
        }
        
        address = person.get("address") or {}
        if isinstance(address, dict):
            profile["location"] = (
                address.get("addressLocality") or 
                address.get("addressRegion") or 
                ""
            )
        
        experience = build_experience(person)
        
        # Se n√£o conseguiu dados suficientes e fallback est√° habilitado
        if use_fallback and (not profile["name"] or not experience):
            print("‚ö†Ô∏è  Insufficient data from scraping, using fallback")
            profile = FALLBACK_DATA["profile"]
            experience = FALLBACK_DATA["experience"]
            education = FALLBACK_DATA["education"]
        else:
            education = []
        
        print(f"‚úì Profile extracted: {profile.get('name', 'Unknown')}")
        print(f"  - Headline: {profile.get('headline', 'N/A')}")
        print(f"  - Experience entries: {len(experience)}")
        
    except Exception as e:
        print(f"‚ùå Error scraping LinkedIn: {e}")
        if use_fallback:
            print("‚úì Using fallback data")
            profile = FALLBACK_DATA["profile"]
            experience = FALLBACK_DATA["experience"]
            education = FALLBACK_DATA["education"]
        else:
            raise

    linkedin_profile = {
        "profile": profile,
        "experience": experience,
        "education": education if 'education' in locals() else [],
    }

    # Preserve existing recommendations instead of overwriting
    recommendations_path = "assets/data/linkedin_recommendations.json"
    if os.path.exists(recommendations_path):
        try:
            with open(recommendations_path, "r", encoding="utf-8") as handle:
                linkedin_recommendations = json.load(handle)
            print("‚úì Preserved existing recommendations")
        except Exception as e:
            print(f"‚ö†Ô∏è  Could not read existing recommendations: {e}")
            linkedin_recommendations = {"recommendations": []}
    else:
        linkedin_recommendations = {"recommendations": []}

    os.makedirs("assets/data", exist_ok=True)
    
    with open("assets/data/linkedin_profile.json", "w", encoding="utf-8") as handle:
        json.dump(linkedin_profile, handle, ensure_ascii=False, indent=2)

    with open(recommendations_path, "w", encoding="utf-8") as handle:
        json.dump(linkedin_recommendations, handle, ensure_ascii=False, indent=2)
    
    print("‚úÖ LinkedIn data saved successfully")


if __name__ == "__main__":
    main()
